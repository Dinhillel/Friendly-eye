<img width="1024" height="1024" alt="logo" src="https://github.com/user-attachments/assets/961e9df4-8088-4eb6-9d65-de36511b245a" />

Assistive Vision System for Visually Impaired people

Project Overview:
This project is an intelligent assistive technology solution designed to help visually impaired people interact with their environment through real-time computer vision and natural language processing. The system combines multiple AI components to provide seamless voice-based interaction with the visual world. Core Functionality The system enables users to ask natural language questions about their surroundings and receive spoken responses. 

For example:
User: "What is in front of me?" System: "There is a red chair and a table in front of you."

âœ¨ Features

ğŸ” Computer Vision Real-time Object Detection: Uses YOLOv8 for accurate object recognition Multi-dataset Training: Trained on COCO and Data-image-captioning datasets Live Camera Feed: Processes video stream in real-time Confidence Scoring: Provides detection confidence levels.

ğŸ™ï¸ Audio Processing Speech-to-Text: Whisper-based voice command recognition Text-to-Speech: Natural voice feedback with pyttsx3 Multi-language Support: Configurable language settings Voice Commands: Interactive voice-controlled interface

ğŸ§  Natural Language Processing Question Answering: T5-based contextual understanding Scene Description: Intelligent interpretation of visual context Conversational Interface: Natural language interaction

âš¡ Performance
Real-time Processing: 30 FPS camera processing GPU Acceleration: CUDA support for faster inference Optimized Models: Lightweight models for efficient processing

 Googel cloud   Maps- for nevegation  

VISION MODEL/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ config/        
â”‚   â”œâ”€â”€ main.py         
â”‚   â””â”€â”€ pipeline.py      
â”‚â”€â”€ vision/
â”‚   â”œâ”€â”€ opencv.py       
â”‚   â””â”€â”€ yolo_detector.py # YOLO object detection
    â””â”€â”€ ocr.py
â”‚â”€â”€ audio/
â”‚   â”œâ”€â”€ stt/             # Speech-to-Text
â”‚   â””â”€â”€ tts/             # Text-to-Speech
â”‚â”€â”€ nlp/
â”‚   â””â”€â”€ t5/              # T5 model for question answering
â”‚â”€â”€Dataset/
Data-image-captioning
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ val/
â”‚   â””â”€â”€ labels/
â”‚       â”œâ”€â”€ train/
â”‚       â””â”€â”€ val/
    â”€â”€ MAPS-             # nevegation street
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md



ğŸ—‚ï¸ Dataset
(https://www.kaggle.com/datasets/aishrules25/automatic-image-captioning-for-visually-impaired)
COCO dataset for object detection

 **Object Detection** using YOLO
-  **Question Answering (NLP)** using T5 model
-  **Speech-to-Text (STT)** using Whisper or another STT module
-  **Text-to-Speech (TTS)** with Coqui TTS / pyttsx3
-  Real-time camera support with OpenCV

git clone https://github.com/Dinhillel/Friendly-eye.git



git clone https://github.com/Dinhillel/Friendly-eye.git


